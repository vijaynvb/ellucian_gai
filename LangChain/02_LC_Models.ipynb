{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tjZg_lD5FCQ"
      },
      "source": [
        "# **Models** - The interface to the AI brains\n",
        "\n",
        "**Problem Statement**\n",
        "\n",
        "Design and implement various AI models using the LangChain and AWS libraries to understand their functionalities and applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FpcUiTJV5JMy"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# update or install the necessary libraries\n",
        "!pip install --upgrade langchain langchain_community langchain_aws langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yc8ym6AS5ULU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = userdata.get('AWS_ACCESS_KEY_ID')\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
        "os.environ[\"AWS_DEFAULT_REGION\"] = userdata.get('AWS_DEFAULT_REGION')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ9GHgim5fHu"
      },
      "source": [
        "## **Language Model**\n",
        "A model that does text in ➡️ text out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aUh4EWQn5oMW"
      },
      "outputs": [],
      "source": [
        "from langchain_aws import BedrockLLM\n",
        "\n",
        "# --- Language Model ---\n",
        "llm = BedrockLLM(\n",
        "    model_id=\"mistral.mistral-7b-instruct-v0:2\",\n",
        "    temperature=0.7,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "qEcTJq0_5rVf",
        "outputId": "3aca1a93-30cc-4448-e6d2-d82cb8736ac5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\nSunday\\n\\nExplanation:\\n\\nIn the seven-day cycle, Friday is followed by Saturday and then Sunday. So, the day that comes after Friday is Sunday. This is the standard answer for the question \"What day comes after Friday?\" However, it\\'s important to note that some cultures may have different naming conventions for their days of the week, so there could be slight variations in the order of the days. But in the vast majority of cases, the answer remains the same: Sunday follows Friday.'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.invoke(\"What day comes after Friday?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-zBN24Z7gjo"
      },
      "source": [
        "## **Chat Model**\n",
        "A model that takes a series of messages and returns a message output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JlnD-App6YXd"
      },
      "outputs": [],
      "source": [
        "from langchain_aws import ChatBedrock\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "# --- Chat Model ---\n",
        "chat = ChatBedrock(\n",
        "    model_id=\"mistral.mistral-7b-instruct-v0:2\",\n",
        "    temperature=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "ukOg4hnU7j_v",
        "outputId": "c7beb14c-78c7-4a8e-9577-35611117caad"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" Well, here's a joke for you: Why don't they sell pencils in New York City? Because it's a pen-insula, get it? I'm here to make your day a little brighter with my wit and humor. But in all seriousness, to get to New York, you should probably book a flight or a train ticket, pack your bags, and make your travel arrangements. Have a nice trip!\""
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = chat.invoke(\n",
        "    [\n",
        "        SystemMessage(content=\"You are an unhelpful AI bot that makes a joke at whatever the user says\"),\n",
        "        HumanMessage(content=\"I would like to go to New York, how should I do this?\")\n",
        "    ]\n",
        ")\n",
        "response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75--VAM6CgvU"
      },
      "source": [
        "## **Function Calling Models**\n",
        "\n",
        "Function calling models are similar to Chat Models but with a little extra flavor. They are fine tuned to give structured data outputs.\n",
        "\n",
        "This comes in handy when you're making an API call to an external service or doing extraction.\n",
        "\n",
        "> - use claude sonnet 3 model for function calling or model which supports function calling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MYPTJoApCgvU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXONa28bCgvU"
      },
      "outputs": [],
      "source": [
        "#from langchain_aws import ChatBedrock\n",
        "\n",
        "#llm = ChatBedrock(\n",
        "#    model_id=\"claude-3.5-sonnet-update this value from bedrock\",\n",
        "#    temperature=1,\n",
        "#)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSliTayHCgvU",
        "outputId": "c805f2cd-ca59-4a94-9ace-e755975a8923"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"location\":\"Boston, MA\"}', 'name': 'get_current_weather'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 89, 'total_tokens': 106, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_07871e2ad8', 'id': 'chatcmpl-BmJTiLnJRpG4dy26mmVE3M5ccJK1y', 'service_tier': 'default', 'finish_reason': 'function_call', 'logprobs': None}, id='run--66287ff8-aae3-4517-84f4-0112a953a1a1-0', usage_metadata={'input_tokens': 89, 'output_tokens': 17, 'total_tokens': 106, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat = ChatOpenAI(model='gpt-4o', temperature=1)\n",
        "\n",
        "output = chat.invoke(input=\n",
        "     [\n",
        "         SystemMessage(content=\"You are an helpful AI bot\"),\n",
        "         HumanMessage(content=\"What’s the weather like in Boston right now?\")\n",
        "     ],\n",
        "     functions=[{\n",
        "         \"name\": \"get_current_weather\",\n",
        "         \"description\": \"Get the current weather in a given location\",\n",
        "         \"parameters\": {\n",
        "             \"type\": \"object\",\n",
        "             \"properties\": {\n",
        "                 \"location\": {\n",
        "                     \"type\": \"string\",\n",
        "                     \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
        "                 },\n",
        "                 \"unit\": {\n",
        "                     \"type\": \"string\",\n",
        "                     \"enum\": [\"celsius\", \"fahrenheit\"]\n",
        "                 }\n",
        "             },\n",
        "             \"required\": [\"location\"]\n",
        "         }\n",
        "     }\n",
        "     ]\n",
        ")\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLUJpftZ83kg"
      },
      "source": [
        "## **Text Embedding Model**\n",
        "Change your text into a vector (a series of numbers that hold the semantic 'meaning' of your text). Mainly used when comparing two pieces of text together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fk8r4-ru8ov6"
      },
      "outputs": [],
      "source": [
        "from langchain_aws import BedrockEmbeddings\n",
        "\n",
        "embeddings = BedrockEmbeddings(\n",
        "    model_id=\"amazon.titan-embed-text-v2:0\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLEMmaN887_P"
      },
      "outputs": [],
      "source": [
        "text = \"Hi! It's time for Learning\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RujU35zb9Dfc",
        "outputId": "550eb936-986c-473e-ff98-cbd1fe1239b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here's a sample: [-0.08459413796663284, 0.08340154588222504, 0.0055779931135475636, 0.0033058084081858397, 0.05176759138703346]...\n",
            "Your embedding is length 1024\n"
          ]
        }
      ],
      "source": [
        "text_embedding = embeddings.embed_query(text)\n",
        "print (f\"Here's a sample: {text_embedding[:5]}...\")\n",
        "print (f\"Your embedding is length {len(text_embedding)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B40kqdxBElD"
      },
      "source": [
        "# **Let's Do an Activity**\n",
        "\n",
        "## **Objective**\n",
        "\n",
        "Create a simple project using the different AI models discussed (language model, chat model and text embedding model) to understand their functionalities and applications.\n",
        "\n",
        "## **Steps**\n",
        "\n",
        "* **Set Up**: Install the necessary libraries and set up your AWS Secret Key and Access Key.\n",
        "* **Language Model**: Implement a language model to generate text based on a given prompt.\n",
        "* **Chat Model**: Implement a chat model to simulate a conversation.\n",
        "* **Text Embedding Model**: Implement a text embedding model to convert text into a vector representation.\n",
        "* **Demonstration**: Provide examples for each model to demonstrate their usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgRbcjcZBhWR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
